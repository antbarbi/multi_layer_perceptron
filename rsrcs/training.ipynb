{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Weight initializers \n",
    "\n",
    "| Initializer       | Best For                | Key Advantage                                      | Major Downside                                 |\n",
    "|-------------------|-------------------------|----------------------------------------------------|-----------------------------------------------|\n",
    "| Zero              | Bias Initialization     | Simplicity                                         | Fails for weights (symmetry problem)          |\n",
    "| Random            | General                 | Breaks symmetry                                    | May cause vanishing/exploding gradients       |\n",
    "| Xavier (Glorot)   | Tanh, Sigmoid           | Keeps variance consistent                          | Not optimal for ReLU                          |\n",
    "| He                | ReLU, Leaky ReLU        | Prevents vanishing/exploding gradients with ReLU   | Can still explode gradients in deep nets      |\n",
    "| LeCun             | SELU                    | Self-normalizing for specific activations          | Limited activation support                    |\n",
    "| Orthogonal        | Recurrent Networks (RNNs)| Maintains long-term gradient flow                  | Only works with square matrices               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>1 - Zero initialization</u>\n",
    "In this method, all weights are initialized to zero.  \n",
    "\n",
    "**<u>Pros:</u>**  \n",
    "- Simple to implement.  \n",
    "- Works well for bias initialization.  \n",
    "\n",
    "**<u>Cons:</u>**  \n",
    "- If all weights are initialized to zero, all neurons in each layer will learn the same thing, effectively making the network symmetric.  \n",
    "- This leads to the model failing to break symmetry, and all neurons will update in the same way, preventing learning.\n",
    "\n",
    "**Conclusion**: This method should not be used for weights, only for biases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>2 - Random initialization</u>\n",
    "Weights are initialized to small random values, often drawn from a uniform or normal distribution.  \n",
    "\n",
    "**<u>Pros:</u>**  \n",
    "- Helps break symmetry, as weights are randomly different for different neurons.\n",
    "\n",
    "**<u>Cons:</u>**  \n",
    "- If the random values are too large, it may lead to exploding gradients (gradients become too large).\n",
    "- If the values are too small, it may lead to vanishing gradients (gradients become too small).\n",
    "\n",
    "**Conclusion:** This method works better than zero initialization but may suffer from issues like vanishing/exploding gradients, especially in deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>3 - Xavier (Glorot) Initialization</u>\n",
    "Weights are initialized using a distribution with zero mean and a variance of \n",
    "$\\frac{1}{n_{in} + n_{out}}$, where $n_{in}$ is the number of input units and $n_{out}$ is the number of output units for the layer. This method works well for sigmoid and tanh activation functions.  \n",
    "\n",
    "- <b>Xavier Uniform</b>: Weights are drawn from a uniform distribution.\n",
    "- <b>Xavier Normal</b>: Weights are drawn from a normal distribution.\n",
    "\n",
    "**<u>Pros:</u>**  \n",
    "- Ensures that the variance of activations and gradients remains consistent across layers, preventing vanishing/exploding gradients.\n",
    "- Good for shallow to moderately deep networks.\n",
    "\n",
    "**<u>Cons:</u>**  \n",
    "- May not work well for very deep networks.\n",
    "- Not optimal for activation functions like ReLU because ReLU can have a different behavior regarding gradient flow compared to tanh or sigmoid.\n",
    "\n",
    "**Conclusion:** Great for activation functions like <b>tanh</b> and <b>sigmoid</b>, but less suited for <b>ReLU</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>4 - He Initialization</u>\n",
    "Similar to Xavier initialization but modified for ReLU and Leaky ReLU activation functions. Weights are initialized using a variance of \n",
    "$\\frac{2}{n_{in}}$, where $n_{in}$ is the number of input units.\n",
    "\n",
    "- <b>He Uniform</b>: Weights are drawn from a uniform distribution.\n",
    "- <b>He Normal</b>: Weights are drawn from a normal distribution.\n",
    "\n",
    "**<u>Pros:</u>**  \n",
    "- Specifically designed for <b>ReLU</b> activation, helping maintain the gradient flow in deep networks.\n",
    "- Prevents vanishing/exploding gradients in deep networks with ReLU activation.\n",
    "\n",
    "**<u>Cons:</u>**  \n",
    "- Can still suffer from exploding gradients in very deep networks.\n",
    "\n",
    "**Conclusion:** Optimal for ReLU-based networks, widely used in modern deep learning architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>5 - LeCun Initialization</u>\n",
    "Weights are initialized using a variance of $\\frac{1}{n_{in}}$, which is good for activation functions like sigmoid or tanh but works best with Leaky ReLU or SELU (Scaled Exponential Linear Units).\n",
    "\n",
    "**<u>Pros:</u>**  \n",
    "- Works well with <b>SELU</b> activation functions.\n",
    "- Ensures that activations don't explode or vanish.\n",
    "\n",
    "**<u>Cons:</u>**  \n",
    "- Limited to specific activation functions, so it's not universally applicable.\n",
    "\n",
    "**Conclusion:** Best for <b>SELU</b> activation, used in self-normalizing neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>6 - Orthogonal Initialization</u>\n",
    "Weights are initialized as orthogonal matrices, typically used in recurrent neural networks (RNNs).  \n",
    "\n",
    "**<u>Pros:</u>**  \n",
    "- Helps avoid exploding and vanishing gradient problems.\n",
    "- Maintains the flow of gradients over long sequences in recurrent networks.\n",
    "\n",
    "**<u>Cons:</u>**  \n",
    "- Only applicable when the number of input and output units is the same (square matrices).\n",
    "- Computationally more complex than other initialization methods.\n",
    "\n",
    "**Conclusion:** Commonly used in RNNs to help maintain long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>7 - Layer-Specific Initialization</u>\n",
    "- <b>Bias Initialization</b>: Often initialized to zero. In some cases, biases may be initialized to small positive values to avoid dead neurons (e.g., for ReLU).\n",
    "\n",
    "**<u>Pros:</u>**  \n",
    "- Sometimes used to \"nudge\" the network toward learning a positive gradient.\n",
    "\n",
    "**<u>Cons:</u>**  \n",
    "- Requires fine-tuning, and the performance benefit can be marginal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

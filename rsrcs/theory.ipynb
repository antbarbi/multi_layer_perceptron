{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "\n",
    "### Weight matrices shape in neural network\n",
    "\n",
    "![Weight matrices shape transformation](images/matrices.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the weights\n",
    "\n",
    "Calculate the gradient of the loss ***L*** with respect to the weight matrix.\n",
    "\n",
    "From the forward pass: \n",
    "$$\n",
    "Z=X‚ãÖW+b\n",
    "$$\n",
    "- ***X*** is the input to the layer.  \n",
    "- ***W*** is the weight matrix.  \n",
    "- ***Z*** is the linear transformation result.   \n",
    "\n",
    "The loss ***L*** depends on ***W***  through  ***Z***. By the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W}\n",
    "$$\n",
    "- $\\frac{\\partial Z}{\\partial L}$ is ùõø (the error signal from the next layer)\n",
    "\n",
    "\n",
    "- $\\frac{\\partial W}{\\partial Z} = X^T$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = X^T \\cdot \\delta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the biases\n",
    "\n",
    "This calculates the gradient of the loss ùêø with respect to the biases ùëè.\n",
    "\n",
    "From the forward pass: \n",
    "$$\n",
    "Z=X‚ãÖW+b\n",
    "$$\n",
    "\n",
    "The bias term ùëè affects ùëç directly. By the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial b}\n",
    "$$\n",
    "\n",
    "Since $\\frac{\\partial Z}{\\partial b} = 1$, we have:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\sum \\delta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "#### Standard\n",
    "\n",
    "This is **gradient descent**:\n",
    "$$\n",
    "W = W - \\eta \\cdot \\nabla L\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $W$: Weight matrix.\n",
    "- $\\eta$: Learning rate.\n",
    "- $\\nabla L$: Gradient of the loss.\n",
    "\n",
    "#### Momentum base optimization\n",
    "\n",
    "This smooth updates:\n",
    "\n",
    "$$\n",
    "v^{(t)} = \\gamma \\cdot v^{(t-1)} - \\eta \\cdot \\nabla L\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $v^{(t)}$: Current velocity.\n",
    "- $\\gamma$: Momentum factor (e.g., 0.9).\n",
    "- $\\eta$: Learning rate.\n",
    "- $\\nabla L$: Gradient of the loss.\n",
    "\n",
    "The weights are then updated using:\n",
    "$$\n",
    "W^{(t+1)} = W^{(t)} + v^{(t)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error signal\n",
    "\n",
    "This calculates the error signal $(\\delta)$ to be passed to the previous layer\n",
    "\n",
    "From the forward pass:\n",
    "$$\n",
    "Z = X \\cdot W + b\n",
    "$$\n",
    "\n",
    "The chain rule for backpropagation states:\n",
    "$$\n",
    "\\delta^{\\text{prev}} = \\frac{\\partial L}{\\partial A^{\\text{prev}}} = \\delta \\cdot W^T\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
